---
layout: post
title: "PCIe BAR and Switch Fabric"
date: 2025-11-07 20:47:00 +0900
categories: [GPU, Network]
tags: [GPU, PCIe, BAR, Switch, RDMA, GDS]
---

ðŸ”**PCIe BAR and Switch Fabric Explained (Component View)**

Modern GPUs and NICs communicate through **PCI Express (PCIe)**.  
Every device on PCIe exposes part of its memory or control registers using **Base Address Registers (BARs)**.  
This post explains how BARs are accessed through a **PCIe switch fabric**, both for **local (same node)** and **remote (RDMA)** communication.

---

## â‘  PCIe Hierarchy Overview

```
Host CPU
 â””â”€ Root Complex (RC)
      â””â”€ PCIe Switch
            â”œâ”€ GPU (Endpoint)
            â””â”€ NIC (Endpoint)
```

- **Root Complex (RC)** is part of the CPU/chipset.  
  It connects system memory and CPUs to the PCIe fabric.  
- **PCIe Switch** connects multiple endpoints such as GPUs and NICs.  
  It acts like a router for PCIe transactions.  
- Each endpoint (GPU, NIC, NVMe) is a **PCIe device** with its own BAR mappings.

---

## â‘¡ Base Address Register (BAR)

**BAR (Base Address Register)** defines a *window* of device memory visible to the PCIe bus.

![PCI BAR memory addresses](https://www.researchgate.net/profile/Jonas-Markussen-2/publication/353106078/figure/fig1/AS:1043973338578944@1625914048636/Device-memory-regions-BARs-are-mapped-into-the-same-address-space-as-system-memory.png)
*Figure 1. Device memory regions (BARs) are mapped into the same address space as system memory.*

- BAR0 â†’ control registers (MMIO)
- BAR1 â†’ device memory window (VRAM or GPU address space)
- BAR2+ â†’ additional memory or configuration spaces

Example for a GPU:
```
BAR0: MMIO registers (control)
BAR1: Device memory window (VRAM)
BAR2/3: Extended 64-bit window
```

When the CPU or another PCIe device wants to access GPU memory,  
it writes to an **address within the GPUâ€™s BAR window** â€” not the GPUâ€™s internal physical address.

> BAR = translation window between PCIe bus addresses and GPU physical memory.

---

## â‘¢ PCIe Switch as a Fabric

![PCIe Switch Architecture](https://fuse.wikichip.org/wp-content/uploads/2018/05/nvidia-dgx-1-v100-nvlink-gpu-xeon-config.png)
*Figure 2. Nvidiaâ€™s NVLink interconnection and the NVSwitch.*

The PCIe switch connects multiple devices using **internal routing tables**.  
It decides which downstream port (GPU, NIC, NVMe) should receive a given PCIe transaction.

```
PCIe Switch
 â”œâ”€ Upstream Port (to Root Complex)
 â”œâ”€ Downstream Port 1 â†’ GPU
 â”œâ”€ Downstream Port 2 â†’ NIC
 â””â”€ Downstream Port 3 â†’ NVMe
```

- Each port has a **Bus Address Range** for its BARs.  
- When the CPU accesses a GPU BAR address,  
  the PCIe switch routes that request to the GPUâ€™s downstream port.

> The switch does not understand "memory" â€” it only routes packets by address range.

---

## â‘£ Local Node Access (CPU â†” GPU BAR)

**Local (intra-node) access** uses the same PCIe fabric and BARs.

```
CPU â†’ PCIe Switch â†’ GPU (BAR1)
```

- The CPU issues a DMA or MMIO write to the GPUâ€™s BAR address.  
- The PCIe switch routes the TLP (Transaction Layer Packet) to the GPU port.  
- The GPU receives it and maps it into its device memory.  
- CUDA runtime or GDS (GPUDirect Storage) uses this path for zero-copy I/O.

> This is how GPUDirect Storage bypasses CPU DRAM and transfers data directly into GPU VRAM.

---

## â‘¤ Remote Node Access (GPU â†” GPU via NIC)

**Remote (inter-node)** access happens through **RDMA** â€” using the NICâ€™s DMA engine.

```
Host CPU
 â””â”€ NIC Driver (mlx5_core)
       â”‚
       â–¼
PCIe Bus
       â”‚
       â–¼
NIC Hardware (HCA)
 â”œâ”€ DMA Engine(s)
 â”œâ”€ Packet Processor
 â”œâ”€ Completion Queue (CQ)
 â””â”€ Doorbell / Queue Pair (QP) Controller
```

Steps:
1. CPU driver (mlx5_core) registers GPU memory and exposes its **Remote BAR (RKey)**.  
2. NICâ€™s **DMA Engine** reads/writes directly to the GPU BAR address over PCIe.  
3. Remote NIC on another node performs the same operation.  
4. Data moves directly GPUâ†”GPU without CPU copy.

> Remote BAR = exported GPU BAR region used for RDMA.

---

## â‘¥ Comparison Summary

| Type | Path | Initiator | Address Type |
|------|------|------------|---------------|
| **CPUâ€“GPU (Local)** | PCIe Switch | CPU | GPU BAR (Local) |
| **GPUâ€“GPU (Local)** | PCIe Switch | GPU DMA | GPU BAR (Peer) |
| **GPUâ€“GPU (Remote)** | PCIe + RDMA | NIC DMA | GPU Remote BAR (RKey) |

---

## â‘¦ BAR + PCIe Fabric Summary

```
[CPU / NIC Driver]
       â†“
PCIe Root Complex
       â†“
PCIe Switch
 â”œâ”€ GPU (BAR)
 â”œâ”€ NIC (DMA Engine)
 â””â”€ NVMe (Controller)
```

- **BAR:** Window that maps GPU or device memory to the PCIe address space.  
- **PCIe Switch:** Routes packets based on BAR address range.  
- **NIC DMA Engine:** Accesses local or remote BARs directly over PCIe or RDMA.  

> The BAR is not memory itself â€” itâ€™s a *bridge address space*  
> between PCIe transactions and real device memory.

---

### Key Takeaways

```
- BAR = PCIe-visible window to device memory
- PCIe Switch = address-based router for TLPs
- Local BAR = used by CPU / peer GPUs
- Remote BAR = used by NIC DMA via RDMA
```

---

*Written by [hdimmfh](https://github.com/hdimmfh)*  
*Nov 2025 â€” Understanding the GPU Stack Series, Part IV*
