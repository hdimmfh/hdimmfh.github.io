---
title: "MPS vs MIG vs MxGPU ‚Äî Modern GPU Virtualization Explained"
by: hdimmfh
date: 2025-11-14 00:20:00 +0900
categories: [GPU, Virtualization]
tags: [MPS, MIG, MxGPU, CUDA, ROCm]
---

üîç**Understanding Modern GPU Virtualization**

> ‚ÄúOne GPU, many jobs ‚Äî but not all sharing is the same.‚Äù \
> In this post, we‚Äôll explore how `NVIDIA MPS`, `VIDIA MIG`, and `AMD MxGPU` differ in design, purpose, and execution model.

---

## ‚ë† Overview

Modern GPUs are no longer single-user accelerators.  
They can now be **shared across multiple processes or even multiple virtual machines**, thanks to technologies like:

- MPS (Multi-Process Service) ‚Äî software-level GPU sharing for CUDA processes  
- MIG (Multi-Instance GPU) ‚Äî hardware-level partitioning on Ampere+ GPUs  
- MxGPU (Multiuser GPU) ‚Äî AMD‚Äôs SR-IOV‚Äìbased GPU virtualization for ROCm environments

---

## ‚ë° MPS ‚Äî Multi-Process Service

`MPS` allows multiple CUDA processes (typically MPI ranks) to share a single GPU concurrently. It replaces CUDA‚Äôs per-process context scheduling with a single shared context, reducing overhead.

‚úîÔ∏è Key traits
- Type: Software-level, runtime multiplexing  
- Scope: Multiple processes of the *same job* (cooperative)  
- Architecture: Client‚Äìserver (via `nvidia-cuda-mps-server`)  
- Benefit: Better utilization, lower context-switch overhead  
- Limitation: Single user per GPU (unless Volta+ `multiuser-server` mode)  

‚úîÔ∏è Memory behavior
- Pre-Volta: Shared GPU virtual address space ‚Üí out-of-range writes could overwrite other clients  
- Volta+: Fully isolated address space per client  

‚úîÔ∏è Best for
- Multi-process CUDA or MPI workloads where each process underutilizes the GPU.

---

## ‚ë¢ MIG ‚Äî Multi-Instance GPU

`MIG` (introduced with NVIDIA Ampere) enables true hardware-level GPU partitioning. Each partition, or GPU instance, has dedicated compute cores, memory, and cache slices.

‚úîÔ∏è Key traits
- Type: Hardware-level virtualization  
- Scope: Independent GPU partitions per job or container  
- Isolation:** Strong ‚Äî each MIG instance behaves like a separate GPU  
- Management: Controlled via `nvidia-smi mig` or Kubernetes device plugin  
- Compatibility: Requires A100 or newer GPUs  

‚úîÔ∏è Memory & performance
- No shared VRAM region; each instance has fixed VRAM capacity.  
- Fault isolation between MIG instances.  
- No context sharing or inter-instance communication.

‚úîÔ∏è Best for
- Multi-tenant environments (Kubernetes, Slurm) where **strict isolation** and **predictable QoS** are required.

---

## ‚ë£ MxGPU ‚Äî AMD‚Äôs Multiuser GPU

`MxGPU (Multiuser GPU)` is AMD‚Äôs SR-IOV‚Äìbased GPU virtualization solution.Unlike NVIDIA‚Äôs MIG or MPS, it‚Äôs designed from the start for virtual machines (VMs).

‚úîÔ∏è Key traits
- Type: Hardware-assisted virtualization (SR-IOV)  
- Scope: Multiple VMs share one physical GPU via virtual functions (VFs)  
- Platform: ROCm + AMD Instinct MI-series or Radeon Pro GPUs  
- Isolation: Each VF has independent memory and scheduler  
- Driver: `amdgpu-pro` with SR-IOV support  

‚úîÔ∏è Integration
- Works seamlessly with **KVM**, **VMware**, and **Proxmox**.  
- Each VF appears as a discrete GPU device to the guest OS.  
- Supports ROCm/HIP workloads on supported hardware.

‚úîÔ∏è Best for
- Cloud and VDI environments where **GPU passthrough per VM** is needed.

---

## ‚öñÔ∏è Comparison Summary

| Feature | **MPS** | **MIG** | **MxGPU** |
|----------|----------|----------|------------|
| Level | Software | Hardware | Hardware (SR-IOV) |
| Vendor | NVIDIA | NVIDIA | AMD |
| Architecture | Client‚ÄìServer | Partitioned SM & VRAM | Virtual Functions (VFs) |
| Isolation | Weak (Pre-Volta)</br> ‚Üí Strong (Volta+) | Strong | Strong |
| Use Case | Multi-process job sharing | Multi-tenant GPU isolation | GPU sharing across VMs |
| CUDA/ROCm | CUDA only | CUDA only | ROCm only |
| Scheduling | Cooperative | Dedicated | Hardware-assisted |
| Typical Env | HPC clusters, MPI | Data centers, Kubernetes | Cloud, Virtualization hosts |

---

## üß≠ Practical Notes

- **MPS** ‚âà ‚Äúmany MPI ranks, one GPU‚Äù  
- **MIG** ‚âà ‚Äúmany GPUs inside one GPU‚Äù  
- **MxGPU** ‚âà ‚Äúone GPU shared across many VMs‚Äù

> ‚ö†Ô∏è These technologies are **mutually exclusive** on a single device.  
> You cannot enable MIG and MPS on the same GPU simultaneously.

---

## üß† TL;DR

| Goal | Recommended Tech |
|------|------------------|
| Maximize GPU utilization for MPI jobs | **MPS** |
| Strong isolation between tenants | **MIG** |
| Virtualization and VM-level GPU sharing | **MxGPU** |

---

## üìö References

- [NVIDIA MPS Documentation](https://docs.nvidia.com/deploy/mps/index.html)  
- [NVIDIA MIG User Guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html)  
- [AMD MxGPU (SR-IOV) Overview](https://www.amd.com/en/technologies/sr-iov.html)  
- [ROCm Documentation](https://rocmdocs.amd.com/en/latest/)  

---

> _Authored by Lucy So_  
> Cloud Infra Engineer & AI Platform Planner  
> [https://soyeonlucy.github.io](https://soyeonlucy.github.io)

