---
title: GPU Memory Architecture â€” The Path of Operands
by: hdimmfh
date: 2025-11-06 10:00:00 +0900
categories: [GPU, GPU Network]
tags: [GPU, HBM, CUDA, Memory, Pipeline]
---

ğŸ”**Understanding the GPU Memory Network**

In the [previous post](https://hdimmfh.github.io/posts/inside-gpu-command-pipeline),  
we traced **how commands** travel â€” from the CUDA driver to SMs.  
Now, to understand **how operands (data)** move,  
we must first understand the **GPU memory network** that physically delivers that data.

Because no instruction runs without its operands,  
understanding the *data path* is just as essential as the *command path.*

---

## â‘  Big Picture â€” The Memory Highway

![NVIDIA A100 GPU Architecture Diagram](https://github.com/hdimmfh/blog-img-repo/blob/main/img/gpu/architecture/CSP-multi-user-with-MIG.png?raw=true)
*Figure 1. NVIDIA A100 GPU Architecture (MIG Example).*

![Memory hierarchy](https://github.com/hdimmfh/blog-img-repo/blob/main/img/gpu/architecture/memory-hierarchy-in-gpus.png?raw=true)
*Figure 2. Memory hierarchy in GPUs.*

From compute cores down to physical DRAM:

```
SMs â†’ L2 Cache â†’ Memory Controllers â†’ HBM Stacks
```

Every byte fetched by a CUDA kernel flows through this route.  
Each layer adds coordination, bandwidth, and parallelism.

---

## â‘¡ Streaming Multiprocessor (SM) â€” Where Computation Starts

```
SM = { CUDA Cores + Tensor Cores + Warp Scheduler + L1 Cache + Registers }
```

- The **SM (Streaming Multiprocessor)** executes kernel instructions.
- Each SM runs thousands of threads in groups of 32 (warps).
- When a warp stalls on memory, another warp is scheduled â€” maximizing utilization.
- The **L1 cache and shared memory** sit closest to execution units.
- Operands first come from registers, then from L1, then from global memory (via L2).

---

## â‘¢ GMMU & L2 Cache â€” Translating and Buffering

```
SM â†’ GMMU â†’ L2 Cache â†’ Memory Controller
```

- The **GMMU (GPU Memory Management Unit)** converts **virtual addresses (GPU VA)** into **physical addresses (PA)**.  
- After translation, the data request hits **L2 Cache**, which is **shared by all SMs**.  
- The L2 acts as a crossroad â€” merging and reordering requests before they go to memory controllers.  
- On a GPU like NVIDIA A100, the L2 is divided into slices connected to each controller via a **high-speed on-die fabric.**

---

## â‘£ Memory Controller â€” The Traffic Dispatcher

```
Controller count = Channel count
```

- Each **Memory Controller** corresponds to **one HBM channel**.  
- It translates memory requests into DRAM-level commands (`ACT`, `READ`, `WRITE`, `PRE`).  
- It handles timing for **bank groups** and **banks**,  
  ensuring proper precharge and activation cycles.  
- Multiple controllers operate in parallel â€” achieving **aggregate terabyte-level bandwidth.**

---

## â‘¤ High-Bandwidth Memory (HBM) â€” The Physical Store

![HBM Internal Structure](https://github.com/hdimmfh/blog-img-repo/blob/main/img/gpu/architecture/HBM-architecture.png?raw=true)
*Figure 3. HBM Stacked DRAM Architecture.*

```
HBM Stack
 â”œâ”€ Channel 0â€“7
 â”‚    â”œâ”€ BankGroup 0â€“7
 â”‚    â”‚    â””â”€ Banks (DRAM cells)
 â””â”€ TSV links (vertical connections)
```

- Each **HBM stack** has **8 independent channels**, each 128-bit wide.  
- 8 Ã— 128 bits = **1024-bit aggregate bus** per stack.  
- Each channel has its own command/address bus and I/O interface.  
- Inside each channel are **bank groups**, each containing multiple **banks** (storage arrays).  
- Data moves vertically between DRAM layers through **TSVs (Through-Silicon Vias)**.

---

## â‘¥ Data Path â€” From Compute to Silicon

```
SMs (Compute)
   â†“
GMMU (Address Translation)
   â†“
L2 Cache (Shared Buffer)
   â†“
Memory Controller (Command Control)
   â†“
HBM Channel â†’ BankGroup â†’ Bank (Physical Storage)
```

- **SMs** generate load/store requests.  
- **GMMU** maps virtual to physical memory.  
- **L2** caches and merges requests from all SMs.  
- **Memory Controllers** issue low-level DRAM commands.  
- **HBM** stores the actual operands that kernels operate on.

Each layer contributes to latency hiding and bandwidth scaling â€”  
a vertical network that bridges software addresses and physical electrons.

---

## â‘¦ Layer Summary

| Layer | Role | Parallelism Unit |
|--------|------|------------------|
| SM | Compute instructions | Warp |
| GMMU | Address translation | Page |
| L2 Cache | Shared buffer | Slice |
| Memory Controller | Command scheduling | Channel |
| HBM | Physical storage | BankGroup / Bank |

---

## â‘§ Why This Matters

In the previous article, commands told the GPU **what to do**.  
In this one, we explored **where the data lives** and **how it moves** when those commands execute.

> Every `ld.global` or `st.global` instruction inside a kernel ultimately traverses  
> the GMMU â†’ L2 â†’ Controller â†’ HBM hierarchy you just saw.

Understanding this path lays the groundwork for the **next post**,  
where weâ€™ll connect both sides â€” command flow and data flow â€”  
to see how **computation and memory** converge on the GPU. âš¡ï¸
