---
title: Command Pipeline â€” From CPU to GPU 
by: hdimmfh
date: 2025-11-04 10:40:00 +0900
categories: [GPU, GPU Network]
tags: [CUDA, PFIFO, PBDMA, GPU, Kernel, CommandBuffer]
---

ðŸ”**Understanding How Commands Travel to the GPU**

When we say *â€œPyTorch sends a kernel to the GPUâ€*, itâ€™s easy to imagine that the CPU somehow pushes arithmetic instructions like `add` or `mul` directly into the GPU.  
But thatâ€™s not what really happens.  

![MMIO Between CPU(Cuda driver) and GPU BAR](https://github.com/hdimmfh/blog-img-repo/blob/main/img/gpu/network/gpu_command_flow.png?raw=true)
*Figure 1. MMIO Between CPU(Cuda driver) and GPU.*

In reality, what travels through PCIe is not a stream of math operations â€”  
itâ€™s a **set of control packets** describing *where in GPU memory the kernel code resides*.

---

## â‘  CUDA Driver â€” The Origin

The **CUDA Driver** acts as a memory manager for the GPU.  
When a kernel is loaded, the driver allocates GPU memory (VRAM), uploads the kernel binary (`.cubin`),  
and already knows its **GPU Virtual Address (GPU VA)** on the device.

So when it builds a **Command Buffer**, the driver doesnâ€™t include the kernelâ€™s instructions â€”  
it writes the **GPU VA where those instructions live**.  
For example:

```
[SET_SHADER_ADDRESS 0x8000_2000]
[SET_GRID_DIM (128,1,1)]
[LAUNCH]
```

The Command Buffer itself lives in VRAM,  
but the addresses inside point to *other* VRAM regions â€” where the real kernel binary sits.  

Thatâ€™s the elegant part:  
> the CPU never sees GPU physical memory directly,  
> but the CUDA Driver already knows all the GPU VAs, because it manages the device memory map.

---

## â‘¡ MMIO and BAR â€” How the CPU Talks to the GPU

To notify the GPU that new commands are ready,  
the driver performs an **MMIO write** to the GPUâ€™s **BAR0 (Base Address Register)** over PCIe.  
This triggers the **doorbell register** in the GPUâ€™s **PFIFO** block â€” the command dispatcher.

In other words, MMIO is simply the CPU sending a "new commands available" signal  
to a memory address that actually corresponds to GPU control logic.

---

## â‘¢ PFIFO â€” The Command Dispatcher

The **PFIFO** hardware inside the GPU manages multiple **Channels** (command queues).  
Each Channel owns its own Command Buffer (base address, GET/PUT pointers).  
Channels correspond to CUDA Streams, processes, or contexts.

PFIFO detects that a Channelâ€™s PUT pointer has advanced,  
and schedules that Channel to a hardware DMA engine called **PBDMA**.  

> Commands inside one Channel are executed *sequentially*,  
> but multiple Channels run *in parallel* â€” this is GPUâ€™s Command-Level Parallelism.

---

## â‘£ PBDMA â€” Fetching Commands from VRAM

The **PBDMA (Push Buffer DMA)** engine performs the real work:  
it reads Command Buffers from VRAM via DMA and decodes their packets.  
Each packet tells it what to do next â€” usually, send the command to a specific **GPU Engine**.

- `LAUNCH` packets â†’ Compute Engine  
- `MEMCOPY` packets â†’ Copy Engine  
- `GRAPHICS` packets â†’ GFX Engine  

Hereâ€™s the surprising part:  
> PBDMA doesnâ€™t hold the kernel instructions themselves.  
> It passes the **GPU Virtual Address (VA)** â€” the pointer to the kernel binary in VRAM â€”  
> to the Compute Engine.

---

## â‘¤ Compute Engine â€” Resolving the Real Kernel

The **Compute Engine** receives this VA,  
consults the **GMMU (GPU Memory Management Unit)** to translate it into a GPU physical address,  
and fetches the kernel binary (SASS code) from VRAM.

![GPU L1 L2 DRAM Architecture](https://github.com/hdimmfh/blog-img-repo/blob/main/img/gpu/architecture/cache_memory_architecture.png?raw=true)
*Figure 2. GPU L1, L2, DRAM Architecture.*

That code then flows down the cache hierarchy:

```
VRAM â†’ L2 Cache â†’ L1 Instruction Cache (per SM)
```

At this point, the kernelâ€™s *actual instructions* â€”  
like `add.f32`, `mul.rn`, or `fma` â€”  
are inside each **Streaming Multiprocessor (SM)** and begin executing in parallel warps.

So the Command Buffer never contained the arithmetic itself â€”  
only the *address* where the arithmetic lives.

---

## â‘¥ From Control to Execution

Letâ€™s recap the full journey:

```
CPU (CUDA Driver)
 â”œâ”€ Builds Command Buffer in VRAM
 â”‚   â””â”€ Contains GPU VAs, not actual code
 â”œâ”€ MMIO Write â†’ GPU BAR0 (doorbell)
 â–¼
PFIFO (Command Dispatcher)
 â”œâ”€ Detects active Channels
 â””â”€ Assigns them to PBDMA
 â–¼
PBDMA (DMA Engine)
 â”œâ”€ Reads Command Buffer from VRAM
 â””â”€ Sends VA to Compute/Copy Engine
 â–¼
Compute Engine
 â”œâ”€ Resolves GPU VA via GMMU
 â”œâ”€ Fetches kernel binary (VRAM â†’ L2 â†’ L1)
 â””â”€ Launches grid
 â–¼
SM
 â””â”€ Executes kernel instructions (add/mul/fmaâ€¦)
```

---

## â‘¦ Serial Inside, Parallel Across

- **Inside one Channel:** Commands are sequential (`GETâ†’PUT` order).  
- **Across Channels:** Commands execute concurrently across multiple PBDMAs and Engines.  

This design allows the GPU to maintain strict ordering within one stream  
while achieving massive concurrency across processes and contexts.

---

## â‘§ What About the Data?

So far, weâ€™ve seen how *commands* travel â€”  
control signals, kernel addresses, and synchronization packets.  

But the story isnâ€™t over.  
> The next big question is:  
> **if commands flow this way, how does the *data* itself travel?**

Thatâ€™s where memory transfers, DMA engines, and GPUDirect paths come into play â€”  
a topic for the next post. ðŸš€

