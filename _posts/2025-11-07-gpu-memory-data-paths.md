---
title: GPU Memory Data Paths â€” How Bytes Actually Move
by: hdimmfh
date: 2025-11-07 10:55:00 +0900
categories: [GPU, GPU Network]
tags: [GPU, PCIe, BAR, Switch, RDMA, GDS, Memory]
---

ðŸ”**GPU Memory Data Paths â€” How Bytes Actually Move**

In the [previous post](https://hdimmfh.github.io/posts/2025-11-06-gpu-memory-architecture),  
we explored **the structure of the GPUâ€™s memory system** â€” how SMs, caches, controllers, and HBM stacks connect.  

![PCIe Switchs Fabric](https://github.com/hdimmfh/blog-img-repo/blob/main/img/gpu/network/pcie_switch_fabric.webp?raw=true)
*Figure 1. PCIe Switchs Fabric.*


Now itâ€™s time to follow the **actual movement of data** through that structure.  
This post focuses on **how bytes travel** across the PCIe fabric, BAR mappings, and DMA engines  
â€” from CPU or NIC all the way into GPU VRAM and HBM.

Understanding these paths completes the picture:  
if the last article explained *where data can live*,  
this one shows *how it actually gets there.*

---

## â‘  PCIe Hierarchy Overview

```
Host CPU
 â””â”€ Root Complex (RC)
      â””â”€ PCIe Switch
            â”œâ”€ GPU (Endpoint)
            â””â”€ NIC (Endpoint)
```

- **Root Complex (RC)** connects CPU/system memory to the PCIe fabric.  
- **PCIe Switch** routes transactions between devices (GPUs, NICs, NVMe).  
- Each endpoint has its own **BARs (Base Address Registers)**, exposing parts of its memory to the PCIe bus.

---

## â‘¡ Base Address Register (BAR) â€” The Window to Device Memory

**BAR (Base Address Register)** defines a memory window that maps a deviceâ€™s internal memory into the PCIe address space.

![PCI BAR memory addresses](https://github.com/hdimmfh/blog-img-repo/blob/main/img/gpu/network/pcie_bar.png?raw=true)
*Figure 2. Device memory regions (BARs) are mapped into the same address space as system memory.*

Example for a GPU:
```
BAR0: MMIO control registers
BAR1: VRAM window (device memory space)
BAR2+: extended or configuration ranges
```

> BAR = a PCIe-visible window into GPU memory, not the memory itself.

When the CPU or another device writes to a BAR address,  
itâ€™s actually sending packets over PCIe that get translated into GPU memory accesses.

---

## â‘¢ PCIe Switch Fabric â€” The Router of Data

The **PCIe Switch** routes all transactions based on their target address range.  
Each downstream port corresponds to a deviceâ€™s BAR region.

```
PCIe Switch
 â”œâ”€ Upstream Port â†’ Root Complex
 â”œâ”€ Downstream Port 1 â†’ GPU
 â”œâ”€ Downstream Port 2 â†’ NIC
 â””â”€ Downstream Port 3 â†’ NVMe
```

When the CPU accesses a GPU BAR address,  
the switch simply forwards that PCIe **Transaction Layer Packet (TLP)** to the correct device.

> The switch doesnâ€™t know what the data means â€” it only routes packets by address.

---

## â‘£ Local Node Transfers â€” CPU â†” GPU

**Intra-node** data movement uses BARs within the same PCIe fabric.

```
CPU â†’ PCIe Root Complex â†’ PCIe Switch â†’ GPU (BAR1 â†’ VRAM)
```

1. The CPU issues a DMA or MMIO write to a GPU BAR address.  
2. The PCIe controller wraps it into **TLPs** (Transaction Layer Packets).  
3. The switch forwards them to the GPU port.  
4. The GPU decodes the BAR address, resolves it to a VRAM offset,  
   and its **DMA engine** performs the actual write into VRAM.

> The CPU never moves the bytes itself â€” it just tells the DMA engine *where* to move them.

This mechanism is what enables **GPUDirect Storage (GDS)** â€”  
NVMe devices can DMA directly into GPU VRAM via BAR1, bypassing host memory entirely.

---

## â‘¤ Peer-to-Peer Transfers â€” GPU â†” GPU (Same Node)

Modern GPUs can also communicate directly over PCIe without CPU involvement.

```
GPU A (BAR) â†” PCIe Switch â†” GPU B (BAR)
```

- Each GPU exposes its memory window through BAR1.  
- The initiating GPU uses its **DMA engine** to write directly into the peerâ€™s BAR address.  
- The switch routes the transaction between GPUs.  
- CUDAâ€™s **peer-to-peer (P2P)** and **NCCL** backends rely on this mechanism.

> BAR-to-BAR transfers are the foundation of **GPUDirect P2P** communication.

---

## â‘¥ RDMA Transfers â€” GPU â†” GPU (Across Nodes)

For **inter-node** transfers, the NIC becomes part of the data path.  
The NICâ€™s **DMA engine** accesses GPU memory via BAR, while another NIC mirrors the process remotely.

```
GPU A (VRAM)
   â†‘
PCIe â†â†’ NIC A â”€â”€â”€â”€ RDMA Fabric â”€â”€â”€â”€ NIC B â†â†’ PCIe
                                               â†“
                                         GPU B (VRAM)
```

Steps:
1. The CPU driver (e.g., `mlx5_core`) registers GPU memory and exposes it as a **Remote BAR (RKey)**.  
2. The NICâ€™s DMA engine performs PCIe reads/writes directly into that BAR region.  
3. The remote NIC does the same, completing **GPUâ†”GPU transfers** with no CPU copy.  

> RDMA turns the BAR window into a globally addressable GPU memory region â€” enabling *true zero-copy* between nodes.

---

## â‘¦ The Full Data Path

```
Host Memory / Storage
   â†“
PCIe Root Complex
   â†“
PCIe Switch Fabric
   â†“
GPU BAR (PCIe-visible address)
   â†“
GPU DMA Engine â†’ Memory Controller â†’ HBM (Physical Data Store)
```

- **CPU or NIC** initiates a PCIe transaction to the GPU BAR address.  
- **PCIe Switch** routes the TLPs to the GPU.  
- **GPUâ€™s DMA Engine** writes data into VRAM.  
- The **Memory Controller** inside the GPU moves it into **HBM** through internal DRAM channels.

Thus, bytes flow from **system or remote memory â†’ PCIe fabric â†’ GPU BAR â†’ HBM stack**.

---

## â‘§ Why It Matters

In the [previous post](https://hdimmfh.github.io/posts/2025-11-04-inside-gpu-command-pipeline),  
we saw how *commands* travel to tell the GPU *what* to do.  
In this post, we examined how *data* actually travels to where itâ€™s needed.  

Together with [GPU Memory Architecture](https://hdimmfh.github.io/posts/2025-11-06-gpu-memory-architecture),  
these layers complete the story of **how commands and data meet** inside the GPU.

> Command Path = PFIFO â†’ PBDMA â†’ SMs  
> Data Path = PCIe BAR â†’ DMA â†’ Memory Controller â†’ HBM

---

## Summary

| Path Type | Initiator | Mechanism | Example |
|------------|------------|------------|----------|
| CPU â†” GPU | CPU DMA | PCIe BAR | GPUDirect Storage |
| GPU â†” GPU (local) | GPU DMA | BAR-to-BAR | NCCL / CUDA P2P |
| GPU â†” GPU (remote) | NIC DMA | RDMA (Remote BAR) | GPUDirect RDMA |

